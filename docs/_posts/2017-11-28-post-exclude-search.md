---
title:  "Dean's Choice Awarded Capstone Project"
header:
  video:
    id: 753615464
    provider: vimeo
# search: false
# categories: 
#  - Jekyll
last_modified_at: 2022-09-25 T08:06:00-05:00
---

In addition to my practice in nuclear research, I also have experience in Software design and development. These skills were honed throughout my final year capstone project, in which I was one part of the 5 student team in developing and manufacturing an automated motion tracking and response system for advanced soccer training.

The goal of this device was to be an accurate, portable, versatile and an accessible means of soccer training for a variety of skill levels. S.T.A.R.S. is an automated ball launching machine capable of tracking a player while pivoting. Coupled with the launcher are four small net-like targets that can detect if a ball passes through the target. From the launcher and target tracking systems, information about how well passing drills are performed (pass speed, pass timing, etc.), will be collected. The system has the added functionality of being able to anticipate player movements and launch a ball to where they are predicted to be, specifically target areas of the body (feet, chest, head). S.T.A.R.S. selectively passes to those areas, which is controllable via voice commands remotely or through a touch screen on the ball launcher. S.T.A.R.S is split up into 9 modules, which are stereoscopic vision, laser range finder, pitch and yaw motor control, launcher motor control, wireless communication, voice control/wearable device, player targets, graphical user interface and central control system.

## Module 1 Stereoscopic
The stereoscopic vision system utilizes two cameras housed on the body of the launcher that provides the system with a large field of view. The tracking system is able to recognize a player and determine the distance in the X and Y planes from the launcher to the player within 0.5 seconds using a colour tracking algorithm. For this to work, the player is required to wear a jersey of a specfic colour. This data is used to rotate and aim the launcher toward the player and adjust the speed of launcher the motors to deliver the ball to the target player with a high degree of accuracy.

Using two cameras, it is easy to calculate the distance of an object without knowing its size. This can be done knowing the focal length of the camera, baseline seperation between the cameras, pixel size of the camera and the disparity value (the difference in pixel position for an object appearing in either camera). As the focal length and pixel size are inherent camera properties and the baseline separation should not change while the camera are in operation, the distance is directly and inversely related to the disparity between the cameras. When disparity is minimized, this correlates to a higher distance, and vice versa.

The higher the pixel resolution of the cameras used, the greater the range of possible disparity values. This results in a higher accuracy and range of distances that can measured. In recognition of the dependency between disparity and pixel size, this relationship was taken into account when considering available camera options. 8 megapixel cameras compatible with the Raspberry Pi were therefore acquired for the purposes of implementing stereo vision.

Normally, a Raspberry Pi only has one ribbon cable slot available to accept a camera module. To work around this limitation, one must either use a proprietary Raspberry Pi board that has multiple camera ports or use multiple Raspberry Pi's with one camera connected to each Pi. The latter strategy was eventually adopted for the purposes of implementation, due to its perceived simplicity over other possible options. Given the focal length of both cameras and the baseline separation between them, the disparity can be determined from an object's shift in position between the two cameras and having the Pi's communicate this information with each other.

Detecting and tracking objects via their colour can be an effective and efficient method for allowing the stereoscopic system to determine a specified target is away from the cameras. If the specific colour values of the target in terms of the colour space used are known to a reasonable degree of certainty, a range of values can be set such that the colour detection algorithm will look for objects or spots in the image that are within that range of set values. In consideration of these values, a mask generated from an input image will have regions that comply with these parameters appear as white, while regions that are outside the specified range will appear as black.

Choosing an appropriate jersey colour and subsequently tuning the HSV values to reflect that colour most accurately will in turn maximise the accuracy of the stereoscopics system, limiting the number of false detections captured by the masking process described above. Additional steps such as Gaussian blurring, erosion, and dilation were also implemented into the algorithm to limit the amount of high-frequency in the mask that may lead to false positives.

The method responsible for monitoring and controlling the stereoscopics currently has each camera identify the largest region of a specific colour present in its field of view, draw a minimum enclosing circle around the region of interest, and then determine using the centre coordinates of the circle the disparity between the two cameras. Using the disparity calculated in this fashion, the distance is them determined. Choosing an appropriate distance to place the cameras apart was influenced by the space available on the platform, the desire to improve the accuracy of the stereoscopics at long distances, and how much of a blind spot right in front of the cameras is allowable. If all other variables are kept constant, increasing the baseline separation increases the possible range of distances that the stereoscopics can track over. This comes at the expense of creating a blind spot right in front of the two cameras. However, as the launcher should never need to deliver a ball right in front of itself, this was deemed an acceptable tradeoff.

To mitigate any possible damage to the cameras, the modules are placed in cases and then mounted using a custom setup. The setup reduces the effects of vibrartions on the cameras using 3D-printed mounting structures and rubber bushings. Any unchecked vibrations can cause the disparity to deviate between the cameras, which can cascade into the system returning inaccurate distance measurements. Mitigating the effects of these vibrations will ensure that the user can have a high degree of confidence in the accuracy of the calculated distance. 

In essence, the operator of this setup can be expressed as shown in the flowchart below. The region of interest is determined by program looking for any contours that match the HSV colour space specifications of the colour in question. Data is transmitted from the client Raspberry Pi to the server Raspberry Pi using an Ethernet connection. The TCP/IP protocol used by the Ethernet ports is proven to be robust, reliable, and fast, which makes it a good choice for real-time communication and data transmission. This speed is necessary for ensuring the most accurate distance measurements are then passed onto be used by other modules on the launcher, and mitigates the difference in processing speed that may arise between the server and client.

## Module 2 Laser Range Finder
In addition to the stereoscopic vision system, a Laser Range Finder (LRF) will be used for a more precise distance measurement, making use of an optical time-of-flight (TOF) modality. This allows for the sensor to only be located on the launcher, without necessitating a receiver on the player. Having redundant distance sensors, especially ones that are each optimal at a certain range, allows for sensor fusion to select the most reliable readings.

A laser range finder emits a pulse of laser at a target. The pulse then reflects off the target and back to the sending device.This TOF principle is based on the fact that laser light travels at a fairly constant speed through the Earth’s atmosphere. The formula for distance is also very straightforward: D = ½ (speed x time).

Initial considerations were to utilize a cheaper and also simpler TOF method as an attempt to innovate a relatively stagnant technology, however the optical TOF distance sensor is a much more capable and robust solution to this problem. The operating frequencies of the optical sensor (GHz) to achieve a more reliable result than the ultrasonic TOF sensors could achieve. Given the complexity of the high frequency circuitry, these sensors are impractical to engineer within the given time frame at the time, thus the two devices were chosen for testing. Interfacing to the device is done via a custom programmable microcontroller that provides serial output, which will be used to obtain the range via a Python function.

Testing of the first device to detect a person in outdoor and indoor conditions showed accurate results between 0-15 meters, but had a difficult time detecting the player beyond this. Tests performed by directing the sensor at a wall or large surface produced more reliable results between 15-30 meters. In practice the maximum range of the device is 30 meters dependent on ambient lighting, while the range for a small target (person ~45 cm wide, 180 cm tall) drops off around 15 meters, due to reduced reflectance of the target.

The second device provided a more reliable result to as much as 25 meters (dependant on the ambient lighting/interference), however the lower FOV will restrict its usability at these ranges due to resolution of the Yaw positioning motor depending on the gear ratio and programming.

The basic theory of the optical TOF sensor is quite similar to that of traditional ultrasonic sensors, the main difference of course being the speed of the wave (C_air). A major challenge in the manufacture of such sensors is the alignment of the optical axis and spacing on the PCB as a result of manufacturing tolerances. As a result, the receiving photodiode must have a greater FOV (field of view) than the transmitter, since the FOV has a great impact on the amount of ambient light and signal collection, there is a clear tradeoff between FOV and performance. The devive we choose is an example of a multi-zone arrangement, an attempt to improve performance while maintaining FOV, uses multiple transmitting LED’s.The second device uses a diode laser (Class 1: CE Certified), giving it a much more narrow FOV, both devices use infrared light at ~950 nm.
 
Due to the extreme directional properties of the above LIDAR devices, the most effective solution was found to be a combination of the two sensors mounted on the front of the launcher. This effectively decreases the directionality of the LIDAR measurements and increases the probability that the beam will fall on the target. In order to determine the devices suitability to this project they must be capable of detecting the player given the resolution of the Yaw motor control.


## Module 3 Pitch & Yaw
The pitch and yaw of the launcher will be controlled by a linear actuator and a planetary gear motor respectively. The accuracy performance will be tested indoors and will be expected to follow a soccer player traveling perpendicular to the launcher, at a maximum speed of 44 km/h, or 12m/s (Usain Bolt's record) at a distance of 10 meters in front of the launcher. With these specifications straining the pitch and yaw motor controls the most, the launcher will be able to follow any person within the range of our sensors.

## Module 4 Ball Launcher
The ball launching system consists of a hopper that is capable of storing five balls that feed into the launching mechanism, which is itself comprised of two motors rotating in opposite directions mounted on either side of a rail to guide the balls. Varying the rotational speed of the launcher motors will modify the speed at which the ball leaves the launcher, and by extension the distance it is able to travel. The central program of the device will determine an appropriate launching speed based on the distance that the player is from the launcher, where on the target (head, chest, feet) the ball is set to be aimed, and the level of speed at which the ball should be travelling when it reaches the player. For stationary targets and moving target, the launcher is be able to launch the ball within a range of 5 to 25 meters. 

## Module 5 Wireless Comunication
Wireless communication for this system is required to communicate data from the player to the launcher, and from the targets to the launcher. Wireless communication in this project is handled by a collection of modules, a type of 2.4 GHz wireless transceiver capable of interfacing with an arduino. The wifi module can be connected to the Arduino using the supply voltage, ground and two supporting pins for Tx/Rx communication.

The wireless module was chosen due to its affordability, extensive catalogue of operating resources, and large maximum operating range of 100 metres (in ideal conditions). It is capable of communicating consistently and with a high success rate via serial communication. Being the principal means of communication between the player and the launcher, initial plans for the wearable envisioned its mounted Arduino communicating directly to the launcher’s onboard Raspberry Pi. After preliminary testing of this design; however, wireless serial communication between the Pi and Arduino was found to have a significantly higher failure rate when compared to direct Arduino-to-Arduino communication. Testing of the Pi-to-Arduino pair was conducted via Arduino code which would transmit a desired string and an appropriate Pi Python code which would receive the transmission, decode the message and print it. The error encountered caused the terminal to display random values following the expected introductory print statement (“Your message is:”), rather than the array of numbers which would then be decoded by the python script into “Hello World!”. Upon further investigation, it is believed that the cause of this failure is rooted in the NRF24L01 Python library: the clocking speed of the module was too high for the Pi to properly process. This theory was later supported with additional testing, by adding a line to the module to change the clock speed.


## Module 6 Voice Control/Wearable Device
Player and device interaction will utilize voice recognition features and a touch screen interface on the launcher. This software is known to be reliable, has shown positive results when tested and allows the voice recognition system on S.T.A.R.S. to achieve a final success rate for processing voice commands at an acceptable rate. This task is made easier through the use of keyword recognition, as opposed to more general recognition. This will include an activation word and preset command phrases for which the system is trained on. The communication between the wearable microphone and launcher will likely result in some lag, meaning the collective system should be able to display a speech to text output on the touchscreen interface within a 5 second period of the user issuing a command.

## Module 7 Player Targets
Targets would be placed around the player, typically in a half moon configuration, these small nets are equipped with break-beam sensors for goal detection. Corresponding red, blue, and green LEDs are used to indicate inactivity, initialization, and successful goal detection respectively. Detection accuracy of this system is 100% due to where the sensors have been placed with the capability of detecting ball speeds up to 15 m/s.

## Module 8 Graphical User Interface
The screen itself will have a main menu for the user to select from a variety of drills. Each drill will possess a submenu for controlling parameters such as ball speed or pass type. 

## Module 9 Central Control System
The main focus and challenge of integrating the device, is the overall control system. Since the stereoscopic system requires two Raspberry Pi's, it is necessary to include the remainder of the systems control onto the Raspberry Pi acting as a server. The reason for this is largely due to the lack of inbuilt Pi to Pi communication making way for us to use the ethernet communication port (using TCP/IP protocol) the easiest method to accomplish this goal. However, each device only has one port. Further complicating the task of control is the need to perform several continuous loops at the same time. The Python language provides many different methods/modules to do this, some of which are: multithreading, multiprocessing, and simply using the import function to add more files. Importing modules and running as processes however, limits the means by which they can share data, while the multiprocessing method is intended for more memory intensive processes. The Python multithreading module allows for multiple tasks to be performed at the same time, while still within the same process and hence the same memory space, allowing the created threads to easily share data.

Though identical in syntax, python processes have certain advantages over threads, specifically when these objects are running tasks that already take advantage of multiple cores, such as basic math and data processing. For example, running mulitple threads that are leveraging the multicore CPU will cause these threads to simply run one after another, but if the task is single core, for example the image processing using the OpenCV library has many single core functions, and so threading the camera processing with a pool of workers would make a lot of sense. As the size and scale of the individual functions has increased: Pitch and Yaw control, Launcher control, and Stereoscopics - running them as threads introduced many anomalies in timing and data output. After changing the syntax to start these functions/class-objects as processes (using the multiprocessing library), the timing issues were largely resolved. 

Since the program uses a combination of processes and threads, some distinction is made between how they each communicate. As mentioned, processes reside in a separate memory space and share nothing with the other process to avoid corruption of data, as result, inputting and outputting data between processes has some caveats. Included in the multiprocessing library are several process safe methods of communication (Queues, Pipes, and Array & Value), the simplest of which is the multiprocessing.Queue() object, chosen for its ease of implementation. Queues from the multiprocessing library differ from those provided in the threading module, for example, they only support a first-in-first-out (FIFO) data structure, and dont allow some of the extra options that are provided in threads (appendleft(), popright(), etc...). This makes passing time sensitive data more complicated, requiring that the current data point is added to the Queue by the producer, only when the consumer is requesting it, this is done by using the Event object provided in multiprocessing.

Communication between threads however is significantly simpler, allowing more complex objects to be passed using Stacks(). Stacks in Python allow for greater flexibility inherent to the data structure. For example, multiple threads and functions can access the same data simultaneously using the peek function, while leaving the data available for later use.

The program structure shown below includes one Python script for the graphical user interface (GUI), and four module files instantiated as processes, which also include several threads. The stereoscopic process simply repeats its image acquisition and comparison loop (~5-8 frames per second) so as not to slow down this critical function the values it detects are pushed to the Pitch and Yaw motor thread, and these two work incongruously to keep the launcher directed at the player at all times. Distance data from the launcher thread. The chaotic appearance of the data flow is largely a result of the availability of serial ports on the Raspberry Pi, since a serial port may not be read asynchronously by two different calls, or even be open in two different data streams. Furthermore, the timing and memory limitations of the Arduino microcontrollers limit where each physical component of the device can be input to the Raspberry Pi. 

In addition to the plethora of data I/O within the program shown, analysis and correction of the data is also very important for the values being fed to the motors, many failsafes and proper shutdown procedures are important. For this reson, shutdown and locking events are implemented using the Event() object from the multiprocessing library. In order to further increase safety and prevent anomalous distance readings from sending large values to the motors when the distance tracking is reading a closer distance most of the time, data smoothing has been implemented using a running averages convolution. Using a small window of five past data points with a kernel size of three, using the Python 3 numpy module which includes a convolution method np.convolve three running averages. From this data, a direction and rate of data change is inferred by taking an average of differences from the past threes means. In doing so, the future data may be naively anticipated in order to sicount anomalous distance readings.

Given the limited processing power of the Raspberry Pi 3B + model, it is required that much of the programming methods be implemented in an efficient manner, making use of object oriented programming methods.
 
